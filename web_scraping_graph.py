"""
StateGraph ì¡°í•© ë°©ì‹ìœ¼ë¡œ ê¸°ì‚¬ ê²€ìƒ‰, ì¶”ì¶œ, ìš”ì•½ì„ ìˆ˜í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
    ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰ ì˜ˆì‹œ: uv run python web_scraping_graph.py \
        --query "ì¸ê³µì§€ëŠ¥ ìµœì‹  ë™í–¥" \
        --num_results 5 \
        --num_sentences 5
    ì¼ìƒ ëŒ€í™” ì˜ˆì‹œ: uv run python web_scraping_graph.py \
        --query "ì•ˆë…•í•˜ì„¸ìš”"
    ê·¸ë˜í”„ ì‹œê°í™”: uv run python web_scraping_graph.py \
        --visualize

ì„¸ë¶€ êµ¬í˜„ ì‚¬í•­:
    - LangGraph StateGraphë¡œ ì „ì²´ ì›Œí¬í”Œë¡œìš° ê´€ë¦¬
    - ì¿¼ë¦¬ ë¶„ë¥˜ â†’ ì¡°ê±´ë¶€ ë¶„ê¸° â†’ ë‰´ìŠ¤ ì²˜ë¦¬ (ê²€ìƒ‰â†’ì¶”ì¶œâ†’ìš”ì•½â†’ì‘ë‹µ) ë˜ëŠ” ì¼ë°˜ ì‘ë‹µ
    - Mermaid diagramì„ í†µí•œ ê·¸ë˜í”„ êµ¬ì¡° ì‹œê°í™” ì§€ì›
"""

import argparse
from typing import Any, Dict, List, Literal, TypedDict

from dotenv import load_dotenv
from duckduckgo_search import DDGS
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph
from langgraph.graph.graph import CompiledGraph
from newspaper import Article

load_dotenv(verbose=True)

# OpenAI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm = ChatOpenAI(model="gpt-4.1-nano", temperature=0)


class StateGraphDebugCallback(BaseCallbackHandler):
    """StateGraph ë…¸ë“œ ì‹¤í–‰ì„ ì¶”ì í•˜ëŠ” callback handlerì…ë‹ˆë‹¤."""

    def __init__(self):
        self.current_node = None
        self.node_count = 0

    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> None:
        """ì²´ì¸(ë…¸ë“œ) ì‹œì‘ ì‹œ í˜¸ì¶œë©ë‹ˆë‹¤."""
        run_name = kwargs.get("name", "Unknown")

        # StateGraph ë…¸ë“œë“¤ë§Œ ì¶”ì 
        valid_nodes = [
            "classify",
            "search_news",
            "fetch_articles",
            "summarize_articles",
            "generate_response",
            "general_response",
        ]
        if run_name in valid_nodes:
            self.current_node = run_name
            self.node_count += 1

            icons = {
                "classify": "ğŸ”",
                "search_news": "ğŸ”",
                "fetch_articles": "ğŸ“„",
                "summarize_articles": "ğŸ“",
                "generate_response": "âœ¨",
                "general_response": "ğŸ’¬",
            }

            print("=" * 50)
            print(f"{icons.get(run_name, 'âš™ï¸')} [{run_name.upper()} ë…¸ë“œ] ì‹¤í–‰ ì‹œì‘")

            # ì…ë ¥ ìƒíƒœ ì¶œë ¥
            if "query" in inputs:
                print(f"ì…ë ¥ ì¿¼ë¦¬: {inputs['query']}")
            if "is_news_related" in inputs:
                print(f"ë‰´ìŠ¤ ê´€ë ¨ì„±: {inputs['is_news_related']}")
            if "news_urls" in inputs and inputs["news_urls"]:
                print(f"ë‰´ìŠ¤ URL ê°œìˆ˜: {len(inputs['news_urls'])}")
            if "articles" in inputs and inputs["articles"]:
                print(f"ì¶”ì¶œëœ ê¸°ì‚¬ ê°œìˆ˜: {len(inputs['articles'])}")
            if "summaries" in inputs and inputs["summaries"]:
                print(f"ìš”ì•½ëœ ê¸°ì‚¬ ê°œìˆ˜: {len(inputs['summaries'])}")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        """ì²´ì¸(ë…¸ë“œ) ì¢…ë£Œ ì‹œ í˜¸ì¶œë©ë‹ˆë‹¤."""
        if self.current_node:
            print(f"âœ… [{self.current_node.upper()} ë…¸ë“œ] ì‹¤í–‰ ì™„ë£Œ")

            # ì¶œë ¥ ìƒíƒœ ì¶œë ¥
            if "is_news_related" in outputs:
                print(f"ë‰´ìŠ¤ ê´€ë ¨ì„± ê²°ê³¼: {outputs['is_news_related']}")
            if "news_urls" in outputs and outputs["news_urls"]:
                print(f"ê²€ìƒ‰ëœ URL ê°œìˆ˜: {len(outputs['news_urls'])}")
            if "articles" in outputs and outputs["articles"]:
                print(f"ì¶”ì¶œëœ ê¸°ì‚¬ ê°œìˆ˜: {len(outputs['articles'])}")
            if "summaries" in outputs and outputs["summaries"]:
                print(f"ìš”ì•½ëœ ê¸°ì‚¬ ê°œìˆ˜: {len(outputs['summaries'])}")
            if "final_response" in outputs and outputs["final_response"]:
                response_len = len(outputs["final_response"])
                print(f"ì‘ë‹µ ê¸¸ì´: {response_len}ì")
                if response_len < 100:
                    print(f"ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {outputs['final_response'][:50]}...")

            print("=" * 50)
            self.current_node = None

    def on_chain_error(
        self,
        error: BaseException,
        *,
        run_id: Any,
        parent_run_id: Any = None,
        **kwargs: Any,
    ) -> None:
        """ì²´ì¸ ì˜¤ë¥˜ ì‹œ í˜¸ì¶œë©ë‹ˆë‹¤."""
        if self.current_node:
            print(f"âŒ [{self.current_node.upper()} ë…¸ë“œ] ì‹¤í–‰ ì˜¤ë¥˜: {error}")
            print("=" * 50)
            self.current_node = None


class NewsScrapingState(TypedDict):
    """
    ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ê·¸ë˜í”„ì˜ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    """

    query: str  # ì‚¬ìš©ì ì¿¼ë¦¬
    is_news_related: bool  # ë‰´ìŠ¤ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€
    news_urls: List[str]  # ê²€ìƒ‰ëœ ë‰´ìŠ¤ URL ëª©ë¡
    articles: List[str]  # ê²€ìƒ‰ëœ ë‰´ìŠ¤ ë³¸ë¬¸ ëª©ë¡
    summaries: List[str]  # ìš”ì•½ëœ ë‰´ìŠ¤ ë³¸ë¬¸ ëª©ë¡
    final_response: str  # ìµœì¢… ì‘ë‹µ ë©”ì‹œì§€
    num_results: int  # ê²€ìƒ‰í•  ë‰´ìŠ¤ ê¸°ì‚¬ ê°œìˆ˜
    num_sentences: int  # ìš”ì•½í•  ë¬¸ì¥ ê°œìˆ˜


def classify_node(state: NewsScrapingState) -> NewsScrapingState:
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ê°€ ë‰´ìŠ¤ ê²€ìƒ‰ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.

    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ

    Returns:
        NewsScrapingState: ë¶„ë¥˜ ê²°ê³¼ê°€ í¬í•¨ëœ ìƒíƒœ
    """
    query = state["query"]

    if not query or not query.strip():
        state["is_news_related"] = False
        return state

    prompt = f"""
    ë‹¤ìŒ ì‚¬ìš©ì ì¿¼ë¦¬ê°€ ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰ì´ í•„ìš”í•œ ë‚´ìš©ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

    ì‚¬ìš©ì ì¿¼ë¦¬: "{query}"

    ë‰´ìŠ¤ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°ì˜ ì˜ˆì‹œ:
    - ìµœì‹  ì‚¬ê±´, ì´ìŠˆ, ë™í–¥ì— ëŒ€í•œ ì§ˆë¬¸
    - íŠ¹ì • íšŒì‚¬, ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ ì†Œì‹
    - "ë‰´ìŠ¤", "ì†Œì‹", "í˜„í™©", "ë™í–¥" ë“±ì˜ í‚¤ì›Œë“œ í¬í•¨

    ë‰´ìŠ¤ ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ê²½ìš°ì˜ ì˜ˆì‹œ:
    - ì¼ë°˜ì ì¸ ì¸ì‚¬ë§ ("ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”")
    - ê°œì¸ì ì¸ ì§ˆë¬¸ì´ë‚˜ ì¼ìƒì ì¸ ëŒ€í™”
    - ê¸°ìˆ ì  ì§ˆë¬¸ì´ë‚˜ í•™ìŠµ ê´€ë ¨ ì§ˆë¬¸
    - ë‚ ì”¨, ìŒì‹ ë“± ì¼ë°˜ì ì¸ ì •ë³´ ì§ˆë¬¸

    "ë„¤" ë˜ëŠ” "ì•„ë‹ˆì˜¤"ë¡œ ì‹œì‘í•´ì„œ í•œ ì¤„ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    """

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        answer = str(response.content).strip().lower()

        # "ë„¤" ë˜ëŠ” "yes"ë¡œ ì‹œì‘í•˜ë©´ ë‰´ìŠ¤ ê´€ë ¨, "ì•„ë‹ˆì˜¤" ë˜ëŠ” "no"ë¡œ ì‹œì‘í•˜ë©´ ì¼ë°˜ ì§ˆë¬¸
        if answer.startswith("ë„¤") or answer.startswith("yes"):
            state["is_news_related"] = True
        elif answer.startswith("ì•„ë‹ˆì˜¤") or answer.startswith("no"):
            state["is_news_related"] = False
        else:
            # ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ìˆ˜í–‰
            state["is_news_related"] = True

    except Exception:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ë‰´ìŠ¤ ê²€ìƒ‰ ìˆ˜í–‰
        state["is_news_related"] = True

    return state


def search_news_node(state: NewsScrapingState) -> NewsScrapingState:
    """
    DuckDuckGo APIë¥¼ ì‚¬ìš©í•´ ë‰´ìŠ¤ ê¸°ì‚¬ URLì„ ê²€ìƒ‰í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.

    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ

    Returns:
        NewsScrapingState: ê²€ìƒ‰ëœ URL ëª©ë¡ì´ í¬í•¨ëœ ìƒíƒœ
    """
    query = state["query"]
    num_results = state["num_results"]

    print(
        f"ê²€ìƒ‰ í‚¤ì›Œë“œ: [{query}]ì— ëŒ€í•´ DuckDuckGo APIë¥¼ ì´ìš©í•´ ê¸°ì‚¬ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤. ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼: {num_results}ê°œ"  # noqa: E501
    )

    ddgs = DDGS()
    urls: List[str] = []

    try:
        results = ddgs.news(query, region="kr-kr", max_results=num_results)
        print(f"ê²€ìƒ‰ ê²°ê³¼: {results}")

        for item in results:
            url = item.get("url") or item.get("href") or item.get("link")
            if url and url.startswith("http"):
                urls.append(url)
                if len(urls) >= num_results:
                    break

        state["news_urls"] = urls

    except Exception as e:
        print(f"DuckDuckGo APIë¥¼ ì´ìš©í•´ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        state["news_urls"] = []

    return state


def fetch_articles_node(state: NewsScrapingState) -> NewsScrapingState:
    """
    ê²€ìƒ‰ëœ URLë“¤ì—ì„œ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.

    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ

    Returns:
        NewsScrapingState: ì¶”ì¶œëœ ê¸°ì‚¬ ë³¸ë¬¸ ëª©ë¡ì´ í¬í•¨ëœ ìƒíƒœ
    """
    urls = state["news_urls"]
    articles: List[str] = []

    for url in urls:
        print(f"URL: {url}ì— ëŒ€í•´ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.")

        try:
            article = Article(url)
            article.download()
            article.parse()

            if article.text and len(article.text.strip()) > 100:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                articles.append(article.text)
            else:
                print(f"URLì—ì„œ ì¶©ë¶„í•œ ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")

        except Exception as e:
            print(f"ê¸°ì‚¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    state["articles"] = articles
    return state


def summarize_articles_node(state: NewsScrapingState) -> NewsScrapingState:
    """
    ì¶”ì¶œëœ ê¸°ì‚¬ë“¤ì„ ê°ê° ìš”ì•½í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.

    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ

    Returns:
        NewsScrapingState: ìš”ì•½ëœ ê¸°ì‚¬ ëª©ë¡ì´ í¬í•¨ëœ ìƒíƒœ
    """
    articles = state["articles"]
    num_sentences = state["num_sentences"]
    summaries: List[str] = []

    for i, text in enumerate(articles):
        if not text or len(text.strip()) == 0:
            summaries.append("ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        print(f"ê¸°ì‚¬ {i + 1}/{len(articles)}: LLMì„ í†µí•´ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ìš”ì•½í•©ë‹ˆë‹¤. ìš”ì•½ ë¬¸ì¥ ê°œìˆ˜: {num_sentences}ê°œ")

        prompt = f"ë‹¤ìŒ ë‰´ìŠ¤ ë³¸ë¬¸ì„ {num_sentences}ê°œì˜ ë¬¸ì¥ ì´ë‚´ë¡œ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n\n{text}\n\nìš”ì•½:"

        try:
            response = llm.invoke([HumanMessage(content=prompt)])
            if isinstance(response.content, str):
                summaries.append(response.content)
            else:
                summaries.append(str(response.content))
        except Exception as e:
            summaries.append(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    state["summaries"] = summaries
    return state


def generate_response_node(state: NewsScrapingState) -> NewsScrapingState:
    """
    ìš”ì•½ëœ ê¸°ì‚¬ë“¤ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.

    Args:
        state: í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ

    Returns:
        NewsScrapingState: ìµœì¢… ì‘ë‹µì´ í¬í•¨ëœ ìƒíƒœ
    """
    query = state["query"]
    summaries = state["summaries"]

    if not summaries:
        state["final_response"] = f"'{query}' ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."
        return state

    # ìš”ì•½ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    combined_summaries = "\n\n".join([f"ê¸°ì‚¬ {i + 1}:\n{summary}" for i, summary in enumerate(summaries)])

    prompt = f"""
    ì‚¬ìš©ìê°€ "{query}"ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤.

    ë‹¤ìŒì€ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ìš”ì•½í•œ ë‚´ìš©ì…ë‹ˆë‹¤:

    {combined_summaries}

    ì´ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¢…í•©ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    ê° ê¸°ì‚¬ì˜ ì£¼ìš” ë‚´ìš©ì„ í¬í•¨í•˜ë˜, ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ì„±ëœ ë‹µë³€ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    """

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        if isinstance(response.content, str):
            state["final_response"] = response.content
        else:
            state["final_response"] = str(response.content)
    except Exception as e:
        state["final_response"] = f"ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    return state


def general_response_node(state: NewsScrapingState) -> NewsScrapingState:
    """
    ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•´ ì‘ë‹µí•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤.

    Args:
        state: í˜„ì¬ ìƒíƒœ

    Returns:
        NewsScrapingState: ì¼ë°˜ ì‘ë‹µì´ í¬í•¨ëœ ìƒíƒœ
    """
    query = state["query"]

    prompt = f"""
    ì‚¬ìš©ìê°€ ë‹¤ìŒê³¼ ê°™ì´ ë§í–ˆìŠµë‹ˆë‹¤: "{query}"

    ì´ëŠ” ë‰´ìŠ¤ ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë‚˜ ëŒ€í™”ì…ë‹ˆë‹¤.
    ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.

    ì˜ˆì‹œ:
    - ì¸ì‚¬ë§ì—ëŠ” ì¸ì‚¬ë¡œ ë‹µí•˜ê¸°
    - ì§ˆë¬¸ì—ëŠ” ë„ì›€ì´ ë˜ëŠ” ì •ë³´ ì œê³µ
    - ëŒ€í™”ë¥¼ ì´ì–´ê°ˆ ìˆ˜ ìˆëŠ” ë°©í–¥ìœ¼ë¡œ ì‘ë‹µ

    ë‹µë³€:
    """

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        if isinstance(response.content, str):
            state["final_response"] = response.content
        else:
            state["final_response"] = str(response.content)
    except Exception as e:
        state["final_response"] = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    return state


def decide_next_step(
    state: NewsScrapingState,
) -> Literal["search_news", "general_response"]:
    """
    ì¿¼ë¦¬ ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ì¡°ê±´ë¶€ ë¶„ê¸° í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        state: í˜„ì¬ ìƒíƒœ

    Returns:
        str: ë‹¤ìŒ ë…¸ë“œ ì´ë¦„ ("search_news" ë˜ëŠ” "general_response")
    """
    # ë¼ìš°íŒ… ì •ë³´ë¥¼ callbackì—ì„œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì¶œë ¥
    print(f"ğŸ”€ [ROUTING] is_news_related={state['is_news_related']} â†’ ", end="")

    if state["is_news_related"]:
        print("search_news")
        return "search_news"
    else:
        print("general_response")
        return "general_response"


def create_news_scraping_graph(num_results: int = 3, num_sentences: int = 3) -> CompiledGraph:
    """
    ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ StateGraphë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        num_results: ê²€ìƒ‰í•  ë‰´ìŠ¤ ê¸°ì‚¬ ê°œìˆ˜
        num_sentences: ìš”ì•½í•  ë¬¸ì¥ ê°œìˆ˜

    Returns:
        CompiledGraph: ì»´íŒŒì¼ëœ StateGraph
    """
    # StateGraph ìƒì„±
    workflow = StateGraph(NewsScrapingState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("classify", classify_node)
    workflow.add_node("search_news", search_news_node)
    workflow.add_node("fetch_articles", fetch_articles_node)
    workflow.add_node("summarize_articles", summarize_articles_node)
    workflow.add_node("generate_response", generate_response_node)
    workflow.add_node("general_response", general_response_node)

    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("classify")
    workflow.add_conditional_edges(
        "classify",
        decide_next_step,
        {"search_news": "search_news", "general_response": "general_response"},
    )

    # ë‰´ìŠ¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì—°ê²°
    workflow.add_edge("search_news", "fetch_articles")
    workflow.add_edge("fetch_articles", "summarize_articles")
    workflow.add_edge("summarize_articles", "generate_response")
    workflow.add_edge("generate_response", END)
    workflow.add_edge("general_response", END)

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    return workflow.compile()


def visualize_graph(graph: CompiledGraph) -> None:
    """
    StateGraphì˜ êµ¬ì¡°ë¥¼ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

    Args:
        graph: ì»´íŒŒì¼ëœ StateGraph ì¸ìŠ¤í„´ìŠ¤
    """
    print("\nğŸ“Š [ê·¸ë˜í”„ êµ¬ì¡° ì‹œê°í™”]")
    print("=" * 80)

    try:
        # Mermaid ë‹¤ì´ì–´ê·¸ë¨ ì¶œë ¥
        print("\nğŸ¨ Mermaid ë‹¤ì´ì–´ê·¸ë¨:")
        print("-" * 50)
        mermaid_code = graph.get_graph().draw_mermaid()
        print(mermaid_code)

        # ASCII ë‹¤ì´ì–´ê·¸ë¨ ì¶œë ¥
        print("\nğŸ“ ASCII ë‹¤ì´ì–´ê·¸ë¨:")
        print("-" * 50)
        ascii_diagram = graph.get_graph().draw_ascii()
        print(ascii_diagram)

        print("\nğŸ’¡ ì°¸ê³ ì‚¬í•­:")
        print("- Mermaid ë‹¤ì´ì–´ê·¸ë¨ì€ https://mermaid.live/ ì—ì„œ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("- ìœ„ ì½”ë“œë¥¼ ë³µì‚¬í•´ì„œ ì˜¨ë¼ì¸ Mermaid ì—ë””í„°ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.")

    except Exception as e:
        print(f"âŒ ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("LangGraph ë²„ì „ì´ë‚˜ ì˜ì¡´ì„±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="StateGraph ê¸°ë°˜ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì—ì´ì „íŠ¸")
    parser.add_argument("--query", type=str, default="ì¸ê³µì§€ëŠ¥ ìµœì‹  ë™í–¥", help="ê²€ìƒ‰í•  í‚¤ì›Œë“œ")
    parser.add_argument("--num_results", type=int, default=3, help="ê²€ìƒ‰í•  ë‰´ìŠ¤ ê¸°ì‚¬ ê°œìˆ˜")
    parser.add_argument("--num_sentences", type=int, default=3, help="ìš”ì•½í•  ë¬¸ì¥ ê°œìˆ˜")
    parser.add_argument("--visualize", action="store_true", help="ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•˜ê³  ì¢…ë£Œ")
    args = parser.parse_args()

    # StateGraph ìƒì„±
    graph = create_news_scraping_graph()

    # ì‹œê°í™” ëª¨ë“œì¸ ê²½ìš° ê·¸ë˜í”„ë§Œ ì¶œë ¥í•˜ê³  ì¢…ë£Œ
    if args.visualize:
        visualize_graph(graph)
        print("\nğŸ¯ ê·¸ë˜í”„ ì‹œê°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        exit(0)

    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = NewsScrapingState(
        query=args.query,
        is_news_related=False,
        news_urls=[],
        articles=[],
        summaries=[],
        final_response="",
        num_results=args.num_results,
        num_sentences=args.num_sentences,
    )

    print(f"ì‹¤í–‰ ì„¤ì •: ê²€ìƒ‰ í‚¤ì›Œë“œ='{args.query}', ê¸°ì‚¬ ê°œìˆ˜={args.num_results}, ìš”ì•½ ë¬¸ì¥ ìˆ˜={args.num_sentences}")
    print("=" * 80)

    # callback handler ìƒì„±
    debug_callback = StateGraphDebugCallback()

    # ê·¸ë˜í”„ ì‹¤í–‰ (callback í¬í•¨)
    result = graph.invoke(initial_state, config={"callbacks": [debug_callback]})

    print("\nğŸ¯ [ìµœì¢… ê²°ê³¼]")
    print(result["final_response"])
